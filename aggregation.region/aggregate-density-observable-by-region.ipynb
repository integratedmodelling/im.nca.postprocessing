{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Aggregation of density observables by region\n",
        "\n",
        "The purpose of this notebook is to serve as interactive documentation for the twin density observable aggregation script. It is structured in a way that the functions are used immediately after they are declared, allowing the user to perform the region-level aggregation process step-by-step. All the functions defined in this notebook are identical to their Python script counterparts. This notebook tackles the concrete example of the aggregation of vegetation carbon stocks at the country level.\n",
        "\n",
        "The script begins by importing all the packages needed to perform the aggregation process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import rasterio.mask\n",
        "from affine import Affine\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input data\n",
        "The script requires two inputs:\n",
        "- the global carbon stock rasters for each year,\n",
        "- the country polygons shapefile.\n",
        "\n",
        "Later in the script, carbon stock values will be aggregated inside each country polygon. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vcs_rasters_directory = r\"\\\\akif.internal\\public\\veg_c_storage_rawdata\" # This is the directory containing carbon stock data. \n",
        "country_polygons_file = r\"\\\\akif.internal\\public\\z_resources\\im-wb\\2015_gaul_dataset_mod_2015_gaul_dataset_global_countries_1.shp\" # This is the shapefile storing the country polygons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the path to the directory containing the carbon stock rasters for each year, the function `get_raster_data` iterates over all the files in the directory and stores the paths inside a list. Later, the script will iterate over this list to load the carbon stock data and perform the country-level aggregation year-by-year. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_raster_data(path):\n",
        "    \"\"\"\n",
        "    get_raster_data gets the addresses of all the raster files (\"*.tif\")\n",
        "    stored in the directory specified by \"path\". Each raster file should\n",
        "    correspond to a different year for the analysis.\n",
        "\n",
        "    :param path: directory containing the raster files for the density observable\n",
        "    to aggregate.\n",
        "    :return: a list storing the addresses of all the raster files.\n",
        "    \"\"\"\n",
        "    file_list = []\n",
        "    for file in os.listdir(path):\n",
        "        # Iterate over all the files in the specified directory.\n",
        "        if \".tif\" in file:\n",
        "            # Process the file if it has a .tif format.\n",
        "            # Build the path according the OS running the script.\n",
        "            if platform.system() is \"Windows\":\n",
        "                address = os.path.join(path, file).replace(\"/\",\"\\\\\")\n",
        "            else:\n",
        "                address = os.path.join(path, file).replace(\"\\\\\",\"/\")\n",
        "                \n",
        "\n",
        "            if address not in file_list:\n",
        "                # Add the file address to the list if it had not been added before.\n",
        "                file_list.append(address)\n",
        "        else:\n",
        "            pass\n",
        "    return file_list\n",
        "\n",
        "vcs_raster_list = get_raster_data(vcs_rasters_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The countries' polygons are the same for each year of the analysis, thus a single shapefile is loaded as a GeoDataFrame with the `load_country_polygons` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_region_polygons(file):\n",
        "    \"\"\"\n",
        "    load_region_polygons loads a shapefile containing a polygon layer describing\n",
        "    the regions to perform the aggregation.\n",
        "\n",
        "    :param file: the address of the shapefile with the region polygons.\n",
        "    :return: a GeoDataFrame with the data on the regions' polygons.\n",
        "    \"\"\"\n",
        "    # Build the path according the OS running the script.\n",
        "    if platform.system() is \"Windows\":\n",
        "        file = file.replace(\"/\",\"\\\\\")\n",
        "    else:\n",
        "        file = file.replace(\"\\\\\",\"/\")\n",
        "        \n",
        "    gdf = gpd.read_file(file)\n",
        "    return gdf\n",
        "\n",
        "country_polygons = load_region_polygons(country_polygons_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregation process\n",
        "\n",
        "This section describes the aggregation process. The aggregation for every year is done inside the function `aggregate_density_observable`, however a number of subprocesses used there are defined in separate functions for clarity. In the following lines, these functions are declared and their purpose explained one-by-one.  \n",
        "\n",
        "### Calculating pixel real-area accounting for pixel latitude\n",
        "\n",
        "The global data produced by ARIES is in tonnes of vegetation carbon per hectare, thus the calculation of the total carbon stock per country in tonnes requires the multiplication of each pixel's value by its area. Although in the planar projection all pixels are identical squares, their real area depends on the pixel's latitude, because of the ellipsoid shape of the Earth. The function `area_of_pixel` calculates the area of a pixel given its side length and the latitude of its center:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def area_of_pixel(pixel_size, center_lat):\n",
        "    \"\"\"\n",
        "    area_of_pixel calculates the area, in hectares, of a wgs84 square raster\n",
        "    tile given its latitude and side-length.\n",
        "    This function is adapted from https://gis.stackexchange.com/a/288034.\n",
        "\n",
        "    :param pixel_size: is the length of the pixel side in degrees.\n",
        "    :param center_lat: is the latitude of the center of the pixel. This value\n",
        "    +/- half the `pixel-size` must not exceed 90/-90 degrees latitude or an\n",
        "    invalid area will be calculated.\n",
        "    :return: the rel area in hectares of a square pixel of side length\n",
        "    `pixel_size` whose center is at latitude `center_lat`.\n",
        "    \"\"\"\n",
        "    a = 6378137  # meters\n",
        "    b = 6356752.3142  # meters\n",
        "    e = math.sqrt(1 - (b/a)**2)\n",
        "    area_list = []\n",
        "    for f in [center_lat+pixel_size/2, center_lat-pixel_size/2]:\n",
        "        zm = 1 - e*math.sin(math.radians(f))\n",
        "        zp = 1 + e*math.sin(math.radians(f))\n",
        "        area_list.append(\n",
        "            math.pi * b**2 * (\n",
        "                math.log(zp/zm) / (2*e) +\n",
        "                math.sin(math.radians(f)) / (zp*zm)))\n",
        "    return (pixel_size / 360. * (area_list[0] - area_list[1])) * np.power(10.0,-4) \n",
        "\n",
        "area_of_pixel_equator      =  area_of_pixel(0.1,0.0)\n",
        "area_of_pixel_tropic       =  area_of_pixel(0.1,23.5)\n",
        "area_of_pixel_polar_circle =  area_of_pixel(0.1,66.33)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating the total carbon stock in a given region \n",
        "\n",
        "Knowing the area of each pixel as a function of its latitude, the calculation of the total carbon stock in a region requires to simply sum for each pixel its value in carbon stock per hectares times its area. The function `aggregate_one_region` performs this operation with the raster `out_image` corresponding to the masked region: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_one_region(out_image, out_transform, pixel_size, width_0, height_0, width_1, height_1):\n",
        "    \"\"\"\n",
        "    aggregate_one_region performs the aggregation of the density observable in a\n",
        "    given region supplied in the form of a masked raster of the observable.\n",
        "\n",
        "    :param out_image: the masked raster layer to aggregate.\n",
        "    :param out_transform: the Affine of the raster layer.\n",
        "    :param pixel_size: the side length in degrees of each square raster tile.\n",
        "    :param width_0: the starting value position of the width array\n",
        "    :param height_0: the starting value position of the height array\n",
        "    :param width_1: the end value position of the width array\n",
        "    :param height_1: the end value position of the height array\n",
        "    :return: the aggregated value of the observable in the specified region.\n",
        "    \"\"\"\n",
        "    # Create a matrix of coordinates based on tile number.\n",
        "    cols, rows = np.meshgrid(np.arange(width_0, width_1), np.arange(height_0, height_1))\n",
        "\n",
        "    # Transform the tile number coordinates to real coordinates and extract only\n",
        "    # latitude information.\n",
        "    ys = rasterio.transform.xy(out_transform, rows, cols)[1] # [0] is xs\n",
        "    latitudes = np.array(ys) # Cast the list of arrays to a 2D array for computational convenience.\n",
        "\n",
        "    # Iterate over the latitudes matrix, calculate the area of each tile, and\n",
        "    # store it in the real_raster_areas array.\n",
        "    real_raster_areas = np.empty(np.shape(latitudes))\n",
        "    for i, latitude_array in enumerate(latitudes):\n",
        "        for j, latitude in enumerate(latitude_array):\n",
        "            real_raster_areas[i,j] = area_of_pixel(pixel_size, latitude)\n",
        "\n",
        "    # Calculate the total value in each tile: density * area = observable value\n",
        "    # in the area.\n",
        "    value = real_raster_areas * out_image[0,height_0:height_1,width_0:width_1] #I don't think np.transpose() is necesary\n",
        "\n",
        "    # Sum all the carbon stock values in the country treating NaNs as 0.0.\n",
        "    aggregated_value = np.nansum(value)\n",
        "\n",
        "    return aggregated_value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "For most of the countries, `aggregate_one_region` is called at the whole-country level, however when the `out_image` raster of the country is too large in size, calling the function results in a buffer overflow.\n",
        "\n",
        "### Splitting large countries in different sections to avoid buffer overflows\n",
        "\n",
        "Some large countries, such as Russia, or the United States, as well as some archipelagic countries require to load in memory rasters that are too large in size, potentially causing memory errors. The purpose of the function `split_and_aggregate` is to avoid memory errors by splitting problematic regions' in smaller square sections when the raster size exceeds 3Gb. Inside the function, the vegetation carbon stock is calculated for each section separately and then summed to obtain the total value for the country:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_and_aggregate(out_image, out_transform, pixel_size, width, height):\n",
        "    \"\"\"\n",
        "    split_and_aggregate splits a raster in tiles of 1000x1000 pixels, performs\n",
        "    the aggregation in each tile and accumulate the results to obtain the total\n",
        "    value for all the region. The function is called when the raster mask\n",
        "    corresponding to the region is too large in size (>3Gb).\n",
        "\n",
        "    :param out_image: is the masked raster layer.\n",
        "    :param out_transform: the Affine containing the transformation matrix with\n",
        "    latitude and longitude values, resolution, etc.\n",
        "    :param pixel_size: is the side length in degrees of each square raster tile.\n",
        "    :param width: is the width of the masked layer.\n",
        "    :param height: is the height of the masked layer.\n",
        "    :return: the value of the aggregated observable at the region-level.\n",
        "    \"\"\"\n",
        "    tilesize = 1000\n",
        "    # The variable to accumulate the aggregated value of the observable.\n",
        "    accumulated_agg_value = 0\n",
        "\n",
        "    for i in range(0, width, tilesize): # Tilesize marks from where to where in width.\n",
        "        for j in range(0, height, tilesize):\n",
        "            # This is for the edge parts, so we don't get nodata from the borders.\n",
        "            w0 = i # Start of the array.\n",
        "            w_plus = min(i+tilesize, width) - i # Addition value.\n",
        "            w1 = w0 + w_plus # End of the array.\n",
        "            h0 = j\n",
        "            h_plus = min(j+tilesize, height) - j\n",
        "            h1 = h0 + h_plus\n",
        "\n",
        "            tile_agg_value = aggregate_one_region(out_image, out_transform, pixel_size, w0, h0, w1, h1)\n",
        "\n",
        "            accumulated_agg_value += tile_agg_value\n",
        "\n",
        "    return accumulated_agg_value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregating carbon stock for every country and every year\n",
        "\n",
        "Finally, the functions declared above are used to perform the aggregation process for every country and every year. The function `aggregate_density_observable` simply iterates over each year's global carbon stock raster file and each country, and masks the global raster file with the polygon corresponding to the country. Then it calculates the aggregated carbon stocks calling `aggregate_one_region` or `split_and_aggregate` when the masked rasters are larger than 3Gb. The results are progressively stored in a DataFrame that is exported to CSV format after every year. When all the years were correctly processed, the function returns the complete DataFrame with the carbon stock per country for every year of the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_density_observable(raster_files_list, region_polygons, temp_export_path):\n",
        "    \"\"\"\n",
        "    aggregate_density_observable aggregates the density observable for all the\n",
        "    raster files specified and inside the specified regions. The result of the\n",
        "    aggregation is returned as a table and the result for each year/raster is\n",
        "    progressively exported in CSV format.\n",
        "\n",
        "    :param raster_files_list: a list containing the addresses of all the raster\n",
        "    files that store the observable's data for each year.\n",
        "    :param region_polygons: a GeoDataFrame storing the polygons corresponding to\n",
        "    each region used for the aggregation.\n",
        "    :param temp_export_path: path to export the temporary results.\n",
        "    :return: a DataFrame storing the aggregated vegetation carbon stocks at the\n",
        "    region level for each year.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Final DataFrame will store the aggregated carbon stocks for each country and each year.\n",
        "    aggregated_df = pd.DataFrame([])\n",
        "\n",
        "    for file in raster_files_list[:]: # [10:]\n",
        "        # Iterate over all the raster files' addresses and extract the year from the address.\n",
        "        filename_length = 24 # This is the number of characters in the raster file name if the convention \"vcs_YYYY_global_300m.tif\" is followed.\n",
        "        start = len(file) - filename_length\n",
        "        year_string_start = file.find(\"vcs_\",start)\n",
        "        file_year = str( file[ year_string_start + 4 : year_string_start + 8] )\n",
        "\n",
        "        print(\"Processing file {} corresponding to year {}.\".format(file, file_year))\n",
        "\n",
        "        # This list will store the results from the aggregation.\n",
        "        aggregated_value_list = []\n",
        "\n",
        "        with rasterio.open(file) as raster_file: # Load the raster file.\n",
        "\n",
        "            gt = raster_file.transform # Get all the raster properties on a list.\n",
        "            pixel_size = gt[0] # X size is stored in position 0, Y size is stored in position 4.\n",
        "            \n",
        "            error_countries_id = [] # Create a list for all encountered possible errors\n",
        "            for row_index, row in region_polygons.iterrows(): # gdf.loc[0:1].iterrows():\n",
        "                try:   \n",
        "                    # Iterate over the country polygons to progressively calculate the total carbon stock in each one of them.\n",
        "\n",
        "                    geo_row = gpd.GeoSeries(row['geometry']) # This is the country's polygon geometry.\n",
        "\n",
        "                    # Masks the raster over the current country. The masking requires two outputs:\n",
        "                    # out_image: the array of the masked image. [z, y, x]\n",
        "                    # out_transform: the Affine containing the transformation matrix with lat / long values, resolution...\n",
        "                    out_image, out_transform = rasterio.mask.mask(raster_file, geo_row, crop=True)\n",
        "\n",
        "                    # Obtain the number of tiles in both directions.\n",
        "                    height = out_image.shape[1]\n",
        "                    width  = out_image.shape[2]\n",
        "\n",
        "                    # Split the masked raster if it is too large to avoid memory\n",
        "                    # errors. Else process the entire region.\n",
        "                    if out_image.nbytes > (3* 10**9):\n",
        "                        print(\"Country {} exceeds 3Gb of memory, splitting the array in tiles of 1000x1000. Current size is GB: {} .\".format(row[\"ADM0_NAME\"], (out_image.nbytes) / np.power(10.0,9)))\n",
        "                        aggregated_value = split_and_aggregate(out_image, out_transform, pixel_size, width, height)\n",
        "\n",
        "                    else:\n",
        "                        aggregated_value = aggregate_one_region(out_image, out_transform, pixel_size, 0, 0, width, height)\n",
        "\n",
        "                except Exception as e:\n",
        "                    # In case there is an error on the process, a value of -9999.0 will be appended\n",
        "                    print(\"the country {} with index {} has errors: {}\".format(row[\"ADM0_NAME\"], row[\"OBJECTID\"], e) )\n",
        "                    error_countries_id.append(row[\"OBJECTID\"])\n",
        "                    aggregated_value = -9999.0\n",
        "                    \n",
        "                # Add the aggregated stock to the list.\n",
        "                aggregated_value_list.append(aggregated_value)\n",
        "\n",
        "                print(\"Country {} finished.\".format(row[\"ADM0_NAME\"]))\n",
        "\n",
        "        print(\"Finished calculating year {}.\".format(file_year))\n",
        "\n",
        "        # Transform the list to a DataFrame using the year as header.\n",
        "        aggregated_observable = pd.DataFrame(aggregated_value_list, columns = [file_year])\n",
        "\n",
        "        # Merge this year's results with the final, multi-year DataFrame.\n",
        "        aggregated_df = pd.merge(aggregated_df, aggregated_observable, how='outer', left_index = True, right_index=True)\n",
        "\n",
        "        # Export the temporary results from curent year.\n",
        "        aggregated_observable.to_csv(temp_export_path + \"_\" + str(file_year) + \".csv\")\n",
        "\n",
        "    return aggregated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output data\n",
        "\n",
        "After the aggregation process is finished, the generated DataFrame is exported to a file in CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_csv(region_polygons, aggregated_observable, path):\n",
        "    \"\"\"\n",
        "    export_to_csv joins the result of the aggregation process for each year with\n",
        "    the regions and exports the final dataset in CSV format to the specified\n",
        "    path.\n",
        "\n",
        "    :param region_polygons: a GeoDataFrame storing the polygons corresponding to\n",
        "    each region used to aggregate.\n",
        "    :param aggregated_observable: a DataFrame storing the aggregated values of\n",
        "    the observable.\n",
        "    :param path: path for the export.\n",
        "    :return: None. The function creates a file in the specified path with the\n",
        "    final results of the aggregation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a DataFrame based on the regions GeoDataFrame and dropping\n",
        "    # unnecessary information in order to keep only: the polygons' Id, region\n",
        "    # codes, and administrative names.\n",
        "    df_final = pd.DataFrame(region_polygons.drop(columns='geometry'))\n",
        "    df_final = df_final.drop([\"STATUS\", \"DISP_AREA\", \"ADM0_CODE\", \"STR0_YEAR\", \"EXP0_YEAR\", \"Shape_Leng\", \"ISO3166_1_\", \"ISO3166__1\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
        "\n",
        "    # Join the depurated regions DataFrame with the aggregated values.\n",
        "    df_final = df_final.join(aggregated_observable)\n",
        "\n",
        "    # Export the result to the specified path.\n",
        "    df_final.to_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the whole script\n",
        "\n",
        "In the Python script the analysis is performed after declaring every function and with the following commands: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The path to the directory containing the raster files with the data on the\n",
        "# density observable to aggregate. # Both Windows and Unix types of path writing\n",
        "# are supported.\n",
        "raster_directory = r\"\\\\akif.internal\\public\\veg_c_storage_rawdata\"\n",
        "\n",
        "# Path to the shapefile containing the data on region polygons that are to be\n",
        "# used in the aggregation.\n",
        "region_polygons_file = r\"\\\\akif.internal\\public\\z_resources\\im-wb\\2015_gaul_dataset_mod_2015_gaul_dataset_global_countries_1.shp\"\n",
        "\n",
        "# Name of the observable to aggregate. This will be used as a prefix for the\n",
        "# temporary results filenames. A suffix \"_YYYY.csv\" will be appended to this\n",
        "# name. The directory for the temporary exports is defined below.\n",
        "observable_name = \"vegetation-carbon-stock\"\n",
        "\n",
        "# Path for the temporal exports of the aggregation process after each raster\n",
        "# processed.\n",
        "temp_export_dir = \"./vegetation.carbon.stock/tmp/vcs.aggregated.country/\"\n",
        "\n",
        "# Path to export the final dataset.\n",
        "export_path = \"./vegetation.carbon.stock/vcs-aggregated-country.csv\"\n",
        "\n",
        "print(\"Loading data.\")\n",
        "raster_list = get_raster_data(raster_directory)\n",
        "region_polygons = load_region_polygons(region_polygons_file)\n",
        "print(\"Data loaded succesfully.\")\n",
        "\n",
        "# Full path for temporary exports.\n",
        "temp_export_path = temp_export_dir + observable_name\n",
        "\n",
        "print(\"Starting aggregation process.\")\n",
        "aggregated_observable = aggregate_density_observable(raster_list, region_polygons, temp_export_path)\n",
        "print(\"Aggregation finished.\")\n",
        "\n",
        "print(\"Exporting the aggregated dataset.\")\n",
        "export_to_csv(region_polygons, aggregated_observable, export_path)\n",
        "print(\"Done.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "73dbe77fa795adcd611523c0e13571b3f618ba9f055bda7fe5e0bb2cbf3117e9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
